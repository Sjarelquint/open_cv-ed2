{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9fc979-2985-4a4c-a347-d9ef18a79a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70f10435-d5bb-4c57-9651-54c0828d4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c58aa99-9865-48bc-a785-2c84e191e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12cb826b-da8f-4770-852e-d707bb4ca472",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    draw_connections(frame,keypoints_with_scores,EDGES,0.4)\n",
    "    #print(keypoints_with_scores)\n",
    "    cv2.imshow('MoveNet Lightning', frame)\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9ff8c8-92ac-4201-a97f-093f4ef0a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 480, 640, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=frame.copy()\n",
    "np.expand_dims(img,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26a0b574-8a83-448d-969f-7b1ee7597f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26443577, 0.4587251 , 0.6275842 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_eye=keypoints_with_scores[0][0][2]\n",
    "right_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dda8b138-7daa-4c1e-aa64-5a93d8e263df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 293])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_eye_coord=np.array(right_eye[:2]*[480,640]).astype(int)\n",
    "right_eye_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "720124a8-21bf-4416-be69-03df279b16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shaped = np.squeeze(np.multiply(interpreter.get_tensor(interpreter.get_output_details()[0]['index']), [480,640,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df67f47c-222c-4858-93e4-b2eaa3159bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.50827522e+02, 3.52420044e+02, 6.31478310e-01],\n",
       "       [1.23691006e+02, 3.92622452e+02, 8.89943659e-01],\n",
       "       [1.26929169e+02, 2.93584061e+02, 6.27584219e-01],\n",
       "       [1.68599453e+02, 4.32411156e+02, 4.97332573e-01],\n",
       "       [1.74786000e+02, 2.30326519e+02, 6.85736299e-01],\n",
       "       [3.13227882e+02, 5.76364861e+02, 6.05513573e-01],\n",
       "       [3.29506416e+02, 9.42217731e+01, 6.80899501e-01],\n",
       "       [3.28126001e+02, 6.26078568e+02, 6.51599541e-02],\n",
       "       [3.99054079e+02, 8.78692627e+00, 4.71896119e-02],\n",
       "       [2.66109295e+02, 5.16138535e+02, 3.76869738e-03],\n",
       "       [2.07124658e+02, 2.00896835e+02, 2.27936562e-02],\n",
       "       [1.41620622e+02, 4.86093864e+02, 1.17531894e-02],\n",
       "       [4.83138542e+02, 1.41929698e+01, 1.06220450e-02],\n",
       "       [3.20947495e+02, 6.20546875e+02, 2.32640877e-02],\n",
       "       [1.67448220e+02, 2.16257324e+02, 1.94503292e-02],\n",
       "       [1.87393727e+02, 4.74290390e+02, 1.98856313e-02],\n",
       "       [1.85094938e+02, 4.24135742e+02, 1.05623584e-02]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f16ba5-2a39-4c60-b91f-9b45686a286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 5, (0,255,0), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48d6f8dd-5235-4adf-ab94-8d641ba77673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee98add-50c4-4f4c-82fe-67e5195961f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
